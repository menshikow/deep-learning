# deep-learning

*This roadmap is inspired by the structure and approach found in [adam-maj/deep-learning](https://github.com/adam-maj/deep-learning).*

## Table of Contents

1. [Chronological Timeline](https://www.google.com/search?q=%23i-chronological-timeline)
2. [Priority Order](https://www.google.com/search?q=%23ii-priority-order)

## **I. Foundations (1943â€“1997)**

*The Era of Logic and Early Neural Networks*

* **1943â€¯|â€¯A Logical Calculus of the Ideas Immanent in Nervous Activity** â€” *McCulloch & Pitts*  
  First formal mathematical model of a neural network.  
  ðŸ“„ [PDF](https://www.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/reasoning/mcp43.pdf)

* **1950â€¯|â€¯Computing Machinery and Intelligence** â€” *Alan Turing*  
  Introduced the Turing Test; posed the question "Can machines think?"  
  ðŸ“„ [PDF](https://www.csee.umbc.edu/courses/471/papers/turing.pdf)

* **1958â€¯|â€¯The Perceptron** â€” *Rosenblatt*  
  First practical algorithm for pattern recognition (single-layer network).

* **1986â€¯|â€¯Learning Representations by Back-Propagating Errors** â€” *Rumelhart, Hinton & Williams*  
  Backpropagation algorithm enabling multi-layer neural networks.  
  ðŸ“„ [PDF](http://www.cs.toronto.edu/~hinton/absps/backprop.pdf)

* **1997â€¯|â€¯Long Short-Term Memory (LSTM)** â€” *Hochreiter & Schmidhuber*  
  Solved vanishing gradient problem in RNNs; introduced memory in sequence learning.  
  ðŸ“„ [PDF](https://www.bioinf.jku.at/publications/older/2604.pdf)

---

## **II. The Deep Learning Explosion (2012â€“2016)**

*Vision, optimization, and deep models*

* **2012â€¯|â€¯ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)** â€” *Krizhevsky et al.*  
  CNNs trained on GPUs revolutionized computer vision.  
  ðŸ“„ [PDF](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)

* **2013â€¯|â€¯Efficient Estimation of Word Representations in Vector Space (Word2Vec)** â€” *Mikolov et al.*  
  Learned continuous vector embeddings capturing semantic word relationships.  
  ðŸ“„ [arXiv:1301.3781](https://arxiv.org/abs/1301.3781)

* **2013â€¯|â€¯Playing Atari with Deep Reinforcement Learning (DQN)** â€” *Mnih et al.*  
  Combined deep learning and RL to play Atari games.  
  ðŸ“„ [arXiv:1312.5602](https://arxiv.org/abs/1312.5602)

* **2014â€¯|â€¯Generative Adversarial Networks (GANs)** â€” *Goodfellow et al.*  
  Generator vs. discriminator for realistic data generation.  
  ðŸ“„ [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)

* **2014â€¯|â€¯Adam: A Method for Stochastic Optimization** â€” *Kingma & Ba*  
  Popular optimizer for neural networks.  
  ðŸ“„ [arXiv:1412.6980](https://arxiv.org/abs/1412.6980)

* **2015â€¯|â€¯Deep Residual Learning for Image Recognition (ResNet)** â€” *He et al.*  
  Skip connections allow very deep networks to train.  
  ðŸ“„ [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)

---

## **III. Transformers & The Age of Scale (2017â€“2020)**

*Attention, pre-training, and emergent behaviors*

* **2017â€¯|â€¯Attention Is All You Need** â€” *Vaswani et al.*  
  Transformer architecture replaces RNNs for sequence modeling.  
  ðŸ“„ [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

* **2017â€¯|â€¯Model-Agnostic Meta-Learning (MAML)** â€” *Finn et al.*  
  Meta-learning framework for fast adaptation to new tasks.  
  ðŸ“„ [arXiv:1703.03400](https://arxiv.org/abs/1703.03400)

* **2018â€¯|â€¯BERT: Pre-training of Deep Bidirectional Transformers** â€” *Devlin et al.*  
  Self-supervised masked language modeling for NLP tasks.  
  ðŸ“„ [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)

* **2019â€¯|â€¯AlphaZero: Mastering Chess and Shogi by Self-Play** â€” *Silver et al.*  
  Generalized AlphaGo self-play approach across multiple games.  
  ðŸ“„ [arXiv:1712.01815](https://arxiv.org/abs/1712.01815)

* **2020â€¯|â€¯Language Models are Few-Shot Learners (GPT-3)** â€” *Brown et al.*  
  Large-scale transformer exhibits few-shot capabilities.  
  ðŸ“„ [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)

* **2020â€¯|â€¯Denoising Diffusion Probabilistic Models (DDPMs)** â€” *Ho et al.*  
  Foundation for modern generative image models (DALL-E 2, Stable Diffusion).  
  ðŸ“„ [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)

---

## **IV. AlphaGo / Game AI Milestones (2016â€“2020)**

*Bridging planning and learning*

* **2016â€¯|â€¯Mastering the Game of Go with Deep Neural Networks and Tree Search (AlphaGo)** â€” *Silver et al.*  
  Neural nets + Monte Carlo Tree Search defeated human champion.  
  ðŸ“„ [Nature](https://www.nature.com/articles/nature16961)

* **2020â€¯|â€¯Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero)** â€” *Schrittwieser et al.*  
  Learns environment model and plans without prior knowledge of rules.  
  ðŸ“„ [arXiv:1911.08265](https://arxiv.org/abs/1911.08265)

---

## **V. Alignment, Scaling, Agents & Structure (2021â€“Present)**

*Emergence, structured representations, and human alignment*

* **2021â€¯|â€¯Geometric Deep Learning** â€” *Bronstein et al.*  
  Unifies CNNs, GNNs, and transformers through geometric structures.

* **2022â€¯|â€¯Chinchilla: Rethinking Model Scaling** â€” *Hoffmann et al.*  
  Scaling laws emphasize data quality and balance with model size.  
  ðŸ“„ [arXiv:2203.15556](https://arxiv.org/abs/2203.15556)

* **2022â€¯|â€¯Training Language Models to Follow Instructions with Human Feedback (InstructGPT)** â€” *Ouyang et al.*  
  RLHF aligns models with human preferences.  
  ðŸ“„ [arXiv:2203.02155](https://arxiv.org/abs/2203.02155)

* **2020s | Scaling Laws for Compute & Performance** â€” *Kaplan et al.*  
  Empirical evidence: scaling compute and data drives performance.  
  ðŸ“„ [arXiv:2001.08361](https://arxiv.org/abs/2001.08361)

* **2020s | Bitter Lesson (Sutton)** â€” *Richard Sutton*  
  Emphasizes general-purpose learning systems over human-engineered heuristics.  
  ðŸ“„ [PDF](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

---

## ðŸš€ **Usage Tip for Repo**

- Copy this Markdown into `README.md` or `timeline.md`.  
- Each entry can be **linked to your own implementation** if you recreate it from scratch.  
- Optionally, add **tags** like `#vision`, `#RL`, `#LLM`, `#meta-learning`, `#generative`.